{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3a0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d631b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices List:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Is built with CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Test for GPU and CUDA\n",
    "\n",
    "print('Devices List: ', tf.config.list_physical_devices('GPU'))\n",
    "print('Is built with CUDA:', tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c250dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List, dictionaries of class number and labels.\n",
    "\n",
    "label_dict = {0: 'Nucleoplasm', 1: 'Nuclear membrane', 2: 'Nucleoli', 3: 'Nucleoli fibrillar center', 4: 'Nuclear speckles', 5: 'Nuclear bodies', 6: 'Endoplasmic reticulum',\n",
    "         7: 'Golgi apparatus', 8: 'Peroxisomes', 9: 'Endosomes', 10: 'Lysosomes', 11: 'Intermediate filaments', 12: 'Actin filaments', 13: 'Focal adhesion sites', \n",
    "         14: 'Microtubules', 15: 'Microtubule ends', 16: 'Cytokinetic bridge', 17: 'Mitotic spindle', 18: 'Microtubule organizing center', 19: 'Centrosome', 20: 'Lipid droplets',\n",
    "         21: 'Plasma membrane', 22: 'Cell junctions', 23: 'Mitochondria', 24: 'Aggresome', 25: 'Cytosol', 26: 'Cytoplasmic bodies', 27: 'Rods & rings'}\n",
    "\n",
    "label_list = ['Nucleoplasm', 'Nuclear membrane', 'Nucleoli', 'Nucleoli fibrillar center', 'Nuclear speckles', 'Nuclear bodies', 'Endoplasmic reticulum', \n",
    "              'Golgi apparatus', 'Peroxisomes', 'Endosomes', 'Lysosomes', 'Intermediate filaments', 'Actin filaments', 'Focal adhesion sites', \n",
    "              'Microtubules', 'Microtubule ends', 'Cytokinetic bridge', 'Mitotic spindle', 'Microtubule organizing center', 'Centrosome', 'Lipid droplets', \n",
    "              'Plasma membrane', 'Cell junctions', 'Mitochondria', 'Aggresome', 'Cytosol', 'Cytoplasmic bodies', 'Rods & rings']\n",
    "\n",
    "map_label_number = {'Nucleoplasm': 0, 'Nuclear membrane': 1, 'Nucleoli': 2, 'Nucleoli fibrillar center': 3, 'Nuclear speckles': 4, 'Nuclear bodies': 5, 'Endoplasmic reticulum': 6, \n",
    "                    'Golgi apparatus': 7, 'Peroxisomes': 8, 'Endosomes': 9, 'Lysosomes': 10, 'Intermediate filaments': 11, 'Actin filaments': 12, 'Focal adhesion sites': 13, \n",
    "                    'Microtubules': 14, 'Microtubule ends': 15, 'Cytokinetic bridge': 16, 'Mitotic spindle': 17, 'Microtubule organizing center': 18, 'Centrosome': 19, 'Lipid droplets': 20, \n",
    "                    'Plasma membrane': 21, 'Cell junctions': 22, 'Mitochondria': 23, 'Aggresome': 24, 'Cytosol': 25, 'Cytoplasmic bodies': 26, 'Rods & rings': 27}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a164f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to train set\n",
    "\n",
    "path_to_train = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2349d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "NUMBER_OF_LABELS = 28\n",
    "\n",
    "NUMBER_OF_CLASSES = 27\n",
    "\n",
    "INPUT_SHAPE = (512, 512, 3) # Images all have the same dimension\n",
    "BATCH_SIZE = 1 # batch size for model learning. 8 gives OOM\n",
    "\n",
    "STEPS_PER_EPOCH = 150\n",
    "VALIDATION_STEPS = 3000\n",
    "NUM_EPOCH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69d34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv('train.csv')\n",
    "ground_truth['List Target'] = ground_truth['Target'].apply(lambda string: sorted(list(map(int, string.split(' ')))))\n",
    "ground_truth['Label Target'] =  ground_truth['List Target'].apply(lambda lis: [label_list[idx] for idx in lis])\n",
    "ground_truth['Label'] =  ground_truth['Label Target'].apply(lambda lis: ', '.join(lis))\n",
    "ground_truth['Path'] = path_to_train + ground_truth['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bae9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train set and validation set\n",
    "\n",
    "train, val = model_selection.train_test_split(ground_truth, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5d7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding\n",
    "\n",
    "def get_clean_data(df):\n",
    "    targets = []\n",
    "    paths = []\n",
    "    for _, row in df.iterrows():\n",
    "        target_np = np.zeros(len(label_list))\n",
    "        t = [int(t) for t in row.Target.split()]\n",
    "        target_np[t] = 1\n",
    "        targets.append(target_np)\n",
    "        paths.append(row.Path)\n",
    "    return np.array(paths), np.array(targets)\n",
    "\n",
    "\n",
    "def load_data(path, target, channels = 3):\n",
    "    if channels == 3:\n",
    "        red = tf.squeeze(tf.image.decode_png(tf.io.read_file(path + '_red.png'), channels = 1), [2])\n",
    "        blue = tf.squeeze(tf.image.decode_png(tf.io.read_file(path + '_blue.png'), channels = 1), [2])\n",
    "        green = tf.squeeze(tf.image.decode_png(tf.io.read_file(path + '_green.png'), channels = 1), [2])\n",
    "        img = tf.stack((red, green, blue), axis=2)\n",
    "    \n",
    "    elif channels == 4:\n",
    "        red = tf.squeeze(tf.image.decode_png(tf.io.read_file(path + '_red.png'), channels = 1), [2])\n",
    "        blue = tf.squeeze(tf.image.decode_png(tf.io.read_file(path + '_blue.png'), channels = 1), [2])\n",
    "        green = tf.squeeze(tf.image.decode_png(tf.io.read_file(path + '_green.png'), channels = 1), [2])\n",
    "        yellow = tf.squeeze(tf.image.decode_png(tf.io.read_file(path + '_yellow.png'), channels = 1), [2])\n",
    "        img = tf.stack((red, green, blue, yellow), axis=2)\n",
    "        \n",
    "    return img, target\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db45d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, train_target = get_clean_data(train)\n",
    "val_path, val_target = get_clean_data(val)\n",
    "\n",
    "#print(f'Train path shape: {train_path.shape}')\n",
    "#print(f'Train target shape: {train_target.shape}')\n",
    "#print(f'Val path shape: {val_path.shape}')\n",
    "#print(f'Val target shape: {val_target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283877ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_data = tf.data.Dataset.from_tensor_slices((train_path, train_target))\n",
    "#val_data = tf.data.Dataset.from_tensor_slices((val_path, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27bfae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = train_data.map(load_data, num_parallel_calls = AUTOTUNE) # prendiamo ogni elemento di train_data (che sono path) e li mappiamo nella rispettiva immagine\n",
    "#val_data = val_data.map(load_data, num_parallel_calls = AUTOTUNE) # <ParallelMapDataset shapes: ((None, None, 3), (28,)), types: (tf.uint8, tf.float64)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100c6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_batches = train_data.batch(BATCH_SIZE).prefetch(buffer_size = AUTOTUNE)\n",
    "#val_data_batches = val_data.batch(BATCH_SIZE).prefetch(buffer_size = AUTOTUNE) # <PrefetchDataset shapes: ((None, None, None, 3), (None, 28)), types: (tf.uint8, tf.float64)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92cee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c928036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_generator(path_target_generator, batch_size = 31072):\n",
    "#def input_generator(path_target_generator):\n",
    "    dt = np.float32\n",
    "    i = 0\n",
    "    while i < batch_size:\n",
    "        path, target = next(path_target_generator)\n",
    "        img, _ = load_data(path, target) \n",
    "        img = np.array(img).astype(dt)\n",
    "\n",
    "        #inputs  = (img, \n",
    "        #           img \n",
    "        #           )\n",
    "\n",
    "        yield (img, img), target\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5145bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_target_generator = itertools.cycle(zip(train_path, train_target))\n",
    "val_path_target_generator = itertools.cycle(zip(val_path, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e470a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (\n",
    "                    (tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32), tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32)),\n",
    "                    tf.TensorSpec(shape=(NUMBER_OF_LABELS), dtype=tf.float32)\n",
    "                    )\n",
    "types = ( (tf.float32,tf.float32),\n",
    "          (tf.float32) )\n",
    "\n",
    "multiple_input_train = tf.data.Dataset.from_generator(lambda: input_generator(train_path_target_generator, len(train_path)), output_types = types)\n",
    "multiple_input_val = tf.data.Dataset.from_generator(lambda: input_generator(val_path_target_generator, len(val_path)), output_types = types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732acb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabec70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b64b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_input_train_batches = multiple_input_train.batch(BATCH_SIZE).prefetch(buffer_size = AUTOTUNE)\n",
    "multiple_input_val_batches = multiple_input_val.batch(BATCH_SIZE).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c206d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9793322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 Workflow\n",
    "\n",
    "resnet_model = keras.applications.ResNet50(include_top = False, weights = 'imagenet') # include_top = False dovrebbe permettermi di personalizzare l'input layer\n",
    "resnet_model.trainable = True\n",
    "\n",
    "res_input = keras.layers.Input(shape = INPUT_SHAPE) # , name = 'Load RGB Images SX'\n",
    "res = resnet_model(res_input)\n",
    "res = keras.layers.Flatten()(res) # create a single array\n",
    "res = keras.layers.Dropout(0.5)(res) # reduce overfitting\n",
    "res = keras.layers.Dense(256, activation = 'relu')(res)\n",
    "res = keras.layers.Dropout(0.5)(res)\n",
    "res_output = keras.layers.Dense(28, activation = 'sigmoid')(res) # 28, one for each class # , name = 'Label_Weights'\n",
    "res_output = keras.layers.Reshape((1,28))(keras.layers.Dense(28, activation = 'sigmoid')(res))\n",
    "#resnet_model = keras.models.Model(res_input, res_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e2615cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception Workflow\n",
    "\n",
    "xception_model = keras.applications.Xception(include_top = False, weights = 'imagenet') # include_top = False dovrebbe permettermi di personalizzare l'input layer\n",
    "xception_model.trainable = True\n",
    "\n",
    "xception_input = keras.layers.Input(shape = INPUT_SHAPE) # , name = 'Load RGB Images DX'\n",
    "xce = xception_model(xception_input)\n",
    "xce = keras.layers.Flatten()(xce) # create a single array\n",
    "xce = keras.layers.Dropout(0.5)(xce) # reduce overfitting\n",
    "xce = keras.layers.Dense(256, activation = 'relu')(xce)\n",
    "xce = keras.layers.Dropout(0.5)(xce)\n",
    "xce_output = keras.layers.Dense(27, activation = 'sigmoid')(xce) # 28, one for each class # , name = 'Cell_Type_Weights'\n",
    "xce_output = keras.layers.Reshape((27,1))(keras.layers.Dense(27, activation = 'sigmoid')(xce))\n",
    "#xception_model = keras.models.Model(xception_input, xce_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c07d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Layer using Dot Product\n",
    "dot_merge = keras.layers.Dot(axes=[1, 2], name = 'Dot_Product')([res_output, xce_output])\n",
    "dot_merge = keras.layers.Flatten()(dot_merge)\n",
    "dot_merge = keras.layers.Dense(378, activation = 'relu')(dot_merge)\n",
    "dot_merge = keras.layers.Dropout(0.5)(dot_merge)\n",
    "hpav100_output = keras.layers.Dense(28, activation = 'sigmoid')(dot_merge) # , name = 'HPAV100_Output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8882c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full HPAV100 Model\n",
    "\n",
    "hpav100_model = keras.models.Model(inputs = [res_input, xception_input], outputs = hpav100_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25cded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Functional)           (None, None, None, 2 23587712    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "xception (Functional)           (None, None, None, 2 20861480    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 524288)       0           resnet50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 524288)       0           xception[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 524288)       0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 524288)       0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          134217984   dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          134217984   dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 28)           7196        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 27)           6939        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 28)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 27, 1)        0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dot_Product (Dot)               (None, 28, 27)       0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 756)          0           Dot_Product[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 378)          286146      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 378)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 28)           10612       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 313,196,053\n",
      "Trainable params: 313,088,405\n",
      "Non-trainable params: 107,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hpav100_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29716025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(hpav100_model, \"hpav100_model.png\", show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9484a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpav100_model.compile(optimizer = keras.optimizers.Adam(1e-4), loss = 'binary_crossentropy', metrics = 'binary_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207938c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "150/150 [==============================] - 208s 1s/step - loss: 0.4761 - binary_accuracy: 0.7831 - val_loss: 0.1870 - val_binary_accuracy: 0.9425\n",
      "Epoch 2/40\n",
      "150/150 [==============================] - 187s 1s/step - loss: 0.2326 - binary_accuracy: 0.9295 - val_loss: 0.1849 - val_binary_accuracy: 0.9352\n",
      "Epoch 3/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.2147 - binary_accuracy: 0.9299 - val_loss: 0.1846 - val_binary_accuracy: 0.9422\n",
      "Epoch 4/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.2161 - binary_accuracy: 0.9338 - val_loss: 0.1931 - val_binary_accuracy: 0.9415\n",
      "Epoch 5/40\n",
      "150/150 [==============================] - 182s 1s/step - loss: 0.2291 - binary_accuracy: 0.9342 - val_loss: 0.1786 - val_binary_accuracy: 0.9422\n",
      "Epoch 6/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.2155 - binary_accuracy: 0.9348 - val_loss: 0.1883 - val_binary_accuracy: 0.9415\n",
      "Epoch 7/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.2088 - binary_accuracy: 0.9346 - val_loss: 0.1792 - val_binary_accuracy: 0.9420\n",
      "Epoch 8/40\n",
      "150/150 [==============================] - 181s 1s/step - loss: 0.2167 - binary_accuracy: 0.9334 - val_loss: 0.1880 - val_binary_accuracy: 0.9398\n",
      "Epoch 9/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1991 - binary_accuracy: 0.9361 - val_loss: 0.1802 - val_binary_accuracy: 0.9418\n",
      "Epoch 10/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1956 - binary_accuracy: 0.9385 - val_loss: 0.1807 - val_binary_accuracy: 0.9421\n",
      "Epoch 11/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1831 - binary_accuracy: 0.9430 - val_loss: 0.1783 - val_binary_accuracy: 0.9415\n",
      "Epoch 12/40\n",
      "150/150 [==============================] - 182s 1s/step - loss: 0.1852 - binary_accuracy: 0.9415 - val_loss: 0.1794 - val_binary_accuracy: 0.9422\n",
      "Epoch 13/40\n",
      "150/150 [==============================] - 182s 1s/step - loss: 0.1954 - binary_accuracy: 0.9358 - val_loss: 0.1822 - val_binary_accuracy: 0.9415\n",
      "Epoch 14/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.2095 - binary_accuracy: 0.9338 - val_loss: 0.1792 - val_binary_accuracy: 0.9425\n",
      "Epoch 15/40\n",
      "150/150 [==============================] - 184s 1s/step - loss: 0.2071 - binary_accuracy: 0.9302 - val_loss: 0.1789 - val_binary_accuracy: 0.9414\n",
      "Epoch 16/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.2016 - binary_accuracy: 0.9376 - val_loss: 0.1796 - val_binary_accuracy: 0.9400\n",
      "Epoch 17/40\n",
      "150/150 [==============================] - 184s 1s/step - loss: 0.1831 - binary_accuracy: 0.9374 - val_loss: 0.1795 - val_binary_accuracy: 0.9360\n",
      "Epoch 18/40\n",
      "150/150 [==============================] - 184s 1s/step - loss: 0.2039 - binary_accuracy: 0.9353 - val_loss: 0.1766 - val_binary_accuracy: 0.9423\n",
      "Epoch 19/40\n",
      "150/150 [==============================] - 184s 1s/step - loss: 0.1954 - binary_accuracy: 0.9372 - val_loss: 0.1800 - val_binary_accuracy: 0.9411\n",
      "Epoch 20/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1900 - binary_accuracy: 0.9350 - val_loss: 0.1787 - val_binary_accuracy: 0.9385\n",
      "Epoch 21/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1960 - binary_accuracy: 0.9387 - val_loss: 0.1782 - val_binary_accuracy: 0.9414\n",
      "Epoch 22/40\n",
      "150/150 [==============================] - 182s 1s/step - loss: 0.1849 - binary_accuracy: 0.9416 - val_loss: 0.1766 - val_binary_accuracy: 0.9421\n",
      "Epoch 23/40\n",
      "150/150 [==============================] - 185s 1s/step - loss: 0.2061 - binary_accuracy: 0.9340 - val_loss: 0.1793 - val_binary_accuracy: 0.9413\n",
      "Epoch 24/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1873 - binary_accuracy: 0.9418 - val_loss: 0.1748 - val_binary_accuracy: 0.9424\n",
      "Epoch 25/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1917 - binary_accuracy: 0.9346 - val_loss: 0.1819 - val_binary_accuracy: 0.9411\n",
      "Epoch 26/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1847 - binary_accuracy: 0.9388 - val_loss: 0.1762 - val_binary_accuracy: 0.9427\n",
      "Epoch 27/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1878 - binary_accuracy: 0.9337 - val_loss: 0.1820 - val_binary_accuracy: 0.9396\n",
      "Epoch 28/40\n",
      "150/150 [==============================] - 183s 1s/step - loss: 0.1923 - binary_accuracy: 0.9362 - val_loss: 0.1773 - val_binary_accuracy: 0.9421\n",
      "Epoch 29/40\n",
      "150/150 [==============================] - 181s 1s/step - loss: 0.1923 - binary_accuracy: 0.9406 - val_loss: 0.1820 - val_binary_accuracy: 0.9411\n",
      "Epoch 30/40\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1897 - binary_accuracy: 0.9324"
     ]
    }
   ],
   "source": [
    "# Uncomment and run for training the model\n",
    "\n",
    "history = hpav100_model.fit(multiple_input_train_batches,\n",
    "                            steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                            validation_data = multiple_input_val_batches,\n",
    "                            validation_steps = VALIDATION_STEPS,\n",
    "                            epochs = NUM_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot History\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(12,4))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpav100_model.save('./hpa_HPAV100_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a34014",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Predict model on validation to get threshold for prediction over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf38db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_table(ground_truth):\n",
    "    all_label = pd.DataFrame(ground_truth['Label Target'].sum(), columns = ['Label']).copy()\n",
    "    frequency_table = all_label['Label'].value_counts().reset_index().copy().rename(columns = {'index': 'Label', 'Label': 'Frequency'})\n",
    "    frequency_table['Numeric Label'] = pd.DataFrame(frequency_table['Label']).replace({'Label': map_label_number}).copy()\n",
    "    frequency_table['Relative Frequency'] = frequency_table['Frequency'] / frequency_table['Frequency'].sum()\n",
    "    frequency_table['Relative Frequency %'] = (frequency_table['Frequency'] / frequency_table['Frequency'].sum()).apply(lambda number: '{:.2%}'.format(number))\n",
    "    frequency_table[['Numeric Label', 'Label', 'Frequency', 'Relative Frequency', 'Relative Frequency %']].set_index('Numeric Label')\n",
    "    return frequency_table\n",
    "\n",
    "def get_threshold_from_validation(model, val_data_batches, ground_truth, batch_size = 4, verbose = 1, steps = None):\n",
    "    frequency_table = get_frequency_table(ground_truth)\n",
    "    prediction = model.predict(val_data_batches, batch_size = BATCH_SIZE, verbose = verbose, steps = steps)\n",
    "    freq = list(frequency_table[['Numeric Label', 'Relative Frequency']].sort_values(by='Numeric Label')['Relative Frequency'])\n",
    "    thresh = np.diagonal(np.array(pd.DataFrame(prediction).quantile([1 - f for f in freq])))\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c312f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_HPAV100 = keras.models.load_model('hpa_HPAV100_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d383ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get threshold for prediction\n",
    "thresh = get_threshold_from_validation(model_HPAV100, multiple_input_val_batches, ground_truth, batch_size = BATCH_SIZE, verbose = 1, steps = None)\n",
    "thresh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATLAS_GPU",
   "language": "python",
   "name": "atlas_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
